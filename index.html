
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
        "http://www.w3.org/TR/html401">
<html>
  <head>
    <title>Weiran Wang</title>
    <link rel="stylesheet" type="text/css" href="html-style.css">
    <meta name="author" content="Weiran Wang">
    <meta name="copyright" content="&copy; 2010 Weiran Wang">
    <meta http-equiv="Content-language" content="en">
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
    <meta name="description" content="Home page of Weiran Wang">
    <meta name="keywords" content="Weiran Wang">
    <meta name="keywords" lang="es" content="Weiran Wang">
    <meta name="robots" content="all">
  </head>
  
  <body>
    
    <table border="0" cellspacing="0">
      <tbody>
	<tr>
	  <td align="left"><img width="360" border="0" src="Pics/weiranwang.jpg" alt="Weiran Wang" title="Weiran Wang, pic taken at home, Jan 2021"></td>
	  <td width="20">&nbsp;</td>
	  <td align="left" nowrap>
	    <h2>Wang, Weiran (&#27754;&#34074;&#28982;)</h2>
		Staff Research Scientist<br>
		Google<br>
		1945 Charleston Rd<br>
		Mountain View, CA 94043<br>
		<br>
		Email: weiran-wang@uiowa.edu<br>
		Email: weiranwang@ttic.edu<br>
		<br>
		<a href="https://ttic.uchicago.edu/~wwang5">previous homepage</a> 
		<br>
	  </td>
	</tr>
      </tbody>
    </table>


<h3>I am joining the <a href=https://cs.uiowa.edu/>CS Department@UIowa</a> as a Tenure-Track Assistant Professor in August 2024. <a href=phd_openings.html>PhD openings</a> are available for Fall 2025.</h3>

<h3>Research Interests</h3>
<p>
I am broadly interested in machine learning algorithms that are well motivated and truly work in practice.
So far, my research topics include multi-modal/multi-view representation learning,
	speech and audio processing, optimization for machine learning, and applications.
</p>

<h3>Biography</h3>
<ul>
<li><p>Jan 2021--June 2024. Senior and Staff Research Scientist. Google.</p></li>
<li><p>Nov 2019--Dec 2020. Senior Research Scientist. Salesforce Research.</p></li>
<li><p>Oct 2017--Oct 2019. Senior Research Scientist. Amazon Alexa.</p></li>
<li><p>Jan 2014--Sep 2017. Postdoc Researcher. <a href="http://ttic.edu">Toyota Technological Institute at Chicago.</a>
	Advisors: <a href="http://ttic.uchicago.edu/~klivescu/">Karen Livescu</a> and <a href="http://ttic.uchicago.edu/~nati/">Nathan Srebro</a></p></li>
<li><p>Aug 2008--Dec 2013. PhD Student. <a href="http://eecs.ucmerced.edu">EECS Department</a> at UC Merced. Advisor: <a href="http://faculty.ucmerced.edu/mcarreira-perpinan/index.html">Miguel A. Carreira-Perpinan</a>.	
</ul>

<h3>Selected Publications</h3>
Full publication list can be found in <a href=https://scholar.google.com/citations?user=O9djN1AAAAAJ&hl=en>google scholar</a>.
<ul>

<li><p>
Weiran Wang, Zelin Wu, Diamantino Caseiro, Tsendsuren Munkhdalai, Khe Chai Sim, Pat Rondon, Golan Pundak, Gan Song, Rohit Prabhavalkar, Zhong Meng, Ding Zhao, Tara Sainath, Pedro Moreno Mengibar.
	Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm.  <em>Interspeech, 2024.</em> <br>
[<a href="https://arxiv.org/abs/2310.00178">arXiv version</a>] </p></li>

<li><p>
Weiran Wang, Rohit Prabhavalkar, Haozhe Shan, Zhong Meng, Dongseong Hwang, Qiujia Li, Khe Chai Sim, Bo Li, James Qin, Xingyu Cai, Adam Stooke, Chengjian Zheng, Yanzhang He, Tara Sainath, Pedro Moreno Mengibar.
	Massive End-to-end Speech Recognition Models with Time Reduction. <em>NAACL, 2024.</em> <br>
[<a href="https://aclanthology.org/2024.naacl-long.344/">link</a>] </p></li>


<li><p>Weiran Wang, Ke Hu, and Tara Sainath. Deliberation of Streaming RNN-Transducer by Non-autoregressive Decoding.  <em> ICASSP, 2022.</em> <br>
[<a href="https://arxiv.org/abs/2112.11442">arXiv version</a>] </p></li>
    
<li><p>Qi Lyu, Xiao Fu, Weiran Wang, and Songtao Lu. Latent Correlation-Based Multiview Learning and Self-Supervision: A Unifying Perspective. <em> ICLR, 2022.</em> <br>
[<a href="https://arxiv.org/abs/2106.07115">arXiv version</a>] </p></li> 
    
<li><p>Junwen Bai, Weiran Wang, and Carla Gomes. Contrastively Disentangled Sequential Variational Autoencoder. <em> NeurIPS, 2021.</em> <br>
[<a href="https://arxiv.org/abs/2110.12091">arXiv version</a>] [<a href="https://github.com/JunwenBai/C-DSVAE">implementation</a>] </p></li> 
    
<li><p>Junwen Bai, Weiran Wang, Yingbo Zhou, and Caiming Xiong. Representation Learning for Sequence Data with Deep Autoencoding Predictive Components. <em>International Conference on Learning Representations (ICLR), 2021.</em> <br>
[<a href="https://arxiv.org/abs/2010.03135">arXiv version</a>] [<a href="https://github.com/JunwenBai/DAPC">implementation</a>] </p></li> 

<li><p>Weiran Wang, Guangsen Wang, Aadyot Bhatnagar, Yingbo Zhou, Caiming Xiong, and Richard Socher. An investigation of phone-based subword units for end-to-end speech recognition. <em>Interspeech, 2020.</em> <br>
<font size="0", color=red>We are the first to investigate phone-based subword units for end-to-end speech recognition.
	For Switchboard, our phone-based BPE system achieves 6.8%/14.4% word error rate (WER) on the Switchboard/CallHome
	portion of the test set while joint decoding achieves 6.3%/13.3% WER. <br>
On Fisher + Switchboard, joint decoding leads to 4.9%/9.5% WER, setting new milestones for telephony speech recognition. </font><br>
[<a href="https://arxiv.org/abs/2004.04290">arXiv version</a>] [<a href="https://github.com/salesforce/transformerasr">implementation</a>] </p></li>
    
<li><p>Yang Chen, Weiran Wang, and Chao Wang. Semi-supervised ASR by End-to-end Self-training. <em>Interspeech, 2020.</em> <br>
<font size="0", color=red>We are one of the first to perform self-training for end-to-end ASR. </font><br>
[<a href="https://arxiv.org/abs/2001.09128">arXiv version</a>] </p></li>
    
<li><p>Weiran Wang, Qingming Tang, and Karen Livescu. Unsupervised Pre-training of Bidirectional Speech Encoders via Masked Reconstruction. <em>ICASSP, 2020.</em><br>
<font size="0", color=red>We are one of the first to perform (BERT-style) masked reconstruction for pre-training speech encoders.</font><br>
[<a href="https://arxiv.org/abs/2001.10603">arXiv version</a>] </p></li>
    
<li><p>Chao Gao, Dan Garber, Nathan Srebro, Jialei Wang, Weiran Wang (by &alpha;-&beta; order). Stochastic Canonical Correlation Analysis. Accepted by <em>Journal of Machine Learning Research.</em><br>
<font size="0", color=red>We provide tight sample complexity analysis of streaming CCA that matches the statistical limit for Gaussian inputs.</font><br>
[<a href="https://arxiv.org/abs/1702.06533">arXiv version</a>] </p></li>

<li><p>Weiran Wang and Nathan Srebro. Stochastic Nonconvex Optimization with Large Minibatches. <em>Algorithmic Learning Theory (ALT), 2019.</em><br>
<font size="0", color=red>We apply proximal point algorithm to non-convex population objective, which boils down to solving a sequence of convex problems,
	and show benefits of larger minibatch size when the loss is not too nonconvex.</font><br>
[<a href="https://arxiv.org/abs/1709.08728">arXiv version</a>] </p></li>

<li><p>Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep Variational Canonical Correlation Analysis. <br>
<font size="0", color=red>We extend the probabilistic interpretation of CCA to use deep generative models, and demonstrate the separation of shared components from private components for multi-view data including audio + articulation, image + text.<br>
[<a href="https://arxiv.org/abs/1610.03454">arXiv version</a>] </p></li>
	
<li><p>Jialei Wang<sup>*</sup>, Weiran Wang<sup>*</sup>, and Nathan Srebro. Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch Prox.
	<em>Conference On Learning Theory (COLT), 2017.</em> <br>
	<font size="0", color=red>We apply the proximal point algorithm to stochastic optimization, which leads to a sequence of regularized ERM problems solved in a distributed setup; this approach offers trade offs between batch size and communication rounds.<br>
[<a href="https://arxiv.org/abs/1702.06269">arXiv version</a>] </p></li>

<li><p>Weiran Wang<sup>*</sup>, Jialei Wang<sup>*</sup>, Dan Garber, and Nathan Srebro. Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis. <em>Advances in Neural Information Processing Systems (NIPS), 2016.</em><br>
<font size="0", color=red>We provide one of the first stochastic optimization algorithms for CCA with global convergence guarantee, where each update makes use of a single data point.</font><br>
[<a href="https://arxiv.org/abs/1604.01870">arXiv version</a>] </p></li>

<li><p>Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On Deep Multi-View Representation Learning. <em>International Conference on Machine Learning (ICML), 2015. </em><br>
<font size="0", color=red>While the deep CCA objective couples all training sample together, we demonstrate that stochastic optimization works well with large minibatch size, and it is a powerful model for extracting the shared component from multi-view data and we propose auto-encoding regularization for it.</font><br>
[<a href="http://ttic.edu/livescu/XRMB_data/full/README">XRMB dataset</a>] [<a href="https://bitbucket.org/qingming_tang/deep-canonical-correlation-analysis/src/master/">Tensorflow implementation!</a>]</p></li>
	
<li><p>Weiran Wang and Miguel A. Carreira-Perpinan. Manifold Blurring Mean Shift Algorithms for Manifold Denoising. <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.</em> </p></li>
</ul>




<!--<h3>Teaching Assistantship</h3>
<ul>
<li><strike>CSE20: Introduction to Computing I, Fall 2012</strike></li>
<li><strike>CSE30: Introduction to Computer Science and Engineering I, Fall 2011</strike></li>
<li><strike>CSE176/EECS276: Machine Learning, Fall 2010</strike></li>
<ul><li><a href="TA/lecture.pdf">(August 27)Classification</a></li><li><a href="TA/P1.pdf">(September 9)Lab Assignment: Principal Component Analysis</a></li></ul>
<li><strike>CSE31: Introduction to Computer Science and Engineering II, Spring 2010</strike></li>
<li><strike><a href="http://johnewart.net/teaching/cse31/">CSE31: Introduction to Computing II, Spring 2010</a></strike></li>
	<ul>
	<li><em>Office hour: Wednesday 3:00-4:00pm, AOB 142</em></li>
	<li><a href="TA/gdb5-refcard.pdf">GDB Reference Card</a></li>
	<li><a href="http://www.scss.tcd.ie/John.Waldron/itral/spim_ref.html">SPIM Quick Reference</a></li>
	<li><a href="TA/minishell.html">Hints for Minishell</a></li>
	<li><a href="http://www.cs.cf.ac.uk/Dave/C/">A good referece for Interprocess Comunication (IPC)</a></li>
	<li><a href="http://beej.us/guide/bgnet/output/html/multipage/index.html">A good reference for Network Programming using Sockets</a></li>
	<li><a href="http://www.tenouk.com/Bufferoverflowc/Bufferoverflow3.html">What's going on in a stack frame</a></li>
	<li><a href="http://tldp.org/HOWTO/NCURSES-Programming-HOWTO/">NCURSES Programming HOWTO</a></li>
	</ul>
<li><strike>CSE31: Introduction to Computer Science and Engineering II, Spring 2009</strike></li>
<li><strike>CSE30: Introduction to Computer Science and Engineering I, Spring 2009</strike></li>
	<ul>
	<li><a href="TA/minesweeper.html">How to use stack for Minesweepers</a></li>
	</ul>
</ul>
-->

<h3>Some Links</h3>
<ul>
<!--<li><a href="manuals.html">Frequently used manuals</a></li>-->
<li><a href="tips.html">Tips for academic life from others</a></li>
</ul>
	  
<hr>

<address> Send email to
<a href="mailto: weiran-wang@uiowa.edu">Weiran Wang</a>.
</address>

<script 
  type="text/JavaScript" 
  language="JavaScript">
//
// format date as dd-mmm-yy
// example: 12-Jan-99
//
function date_ddmmmyy(date)
{
  var d = date.getDate();
  var m = date.getMonth() + 1;
  var y = date.getYear();

  // handle different year values 
  // returned by IE and NS in 
  // the year 2000.
  if(y >= 2000)
  {
    y -= 2000;
  }
  if(y >= 100)
  {
    y -= 100;
  }

  // could use splitString() here 
  // but the following method is 
  // more compatible
  var mmm = 
    ( 1==m)?'Jan':( 2==m)?'Feb':(3==m)?'Mar':
    ( 4==m)?'Apr':( 5==m)?'May':(6==m)?'Jun':
    ( 7==m)?'Jul':( 8==m)?'Aug':(9==m)?'Sep':
    (10==m)?'Oct':(11==m)?'Nov':'Dec';

  return "" +
    (d<10?"0"+d:d) + "-" +
    mmm + "-" +
    (y<10?"0"+y:y);
}


//
// get last modified date of the 
// current document.
//
function date_lastmodified()
{
  var lmd = document.lastModified;
  var s   = "Unknown";
  var d1;

  // check if we have a valid date
  // before proceeding
  if(0 != (d1=Date.parse(lmd)))
  {
    s = "" + date_ddmmmyy(new Date(d1));
  }

  return s;
}

//
// finally display the last modified date
// as DD-MMM-YY
//
document.write("This page was updated on " + date_lastmodified() + ".");

//
</script>
    
  </body>
</html>
