
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
        "http://www.w3.org/TR/html401">
<html>
  <head>
    <title>Weiran Wang</title>
    <link rel="stylesheet" type="text/css" href="html-style.css">
    <meta name="author" content="Weiran Wang">
    <meta name="copyright" content="&copy; 2010 Weiran Wang">
    <meta http-equiv="Content-language" content="en">
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
    <meta name="description" content="Home page of Weiran Wang">
    <meta name="keywords" content="Weiran Wang">
    <meta name="keywords" lang="es" content="Weiran Wang">
    <meta name="robots" content="all">
  </head>
  
  <body>
    
    <table border="0" cellspacing="0">
      <tbody>
	<tr>
	  <td align="left"><img width="360" border="0" src="Pics/weiranwang.jpg" alt="Weiran Wang" title="Weiran Wang, pic taken at home, Jan 2021"></td>
	  <td width="20">&nbsp;</td>
	  <td align="left" nowrap>
	    <h2>Wang, Weiran (&#27754;&#34074;&#28982;)</h2>
		Assistant Professor<br>
		<a href="https://cs.uiowa.edu/">Computer Science Department</a><br>
		<a href="https://uiowa.edu/">University of Iowa</a><br>
		<br>
		Email : weiran-wang@uiowa.edu<br>
		<br>
		<!--Email: weiranwang@ttic.edu<br>-->
		201L MacLean Hall<br>
		2 W Washington St, Iowa City, IA 52240<br>
		Office Phone : 319-467-1886<br>
		<a href="https://ttic.uchicago.edu/~wwang5">previous homepage</a> 
		<br>
	  </td>
	</tr>
      </tbody>
    </table>


<!--<h3><a href=phd_openings.html>PhD openings</a> are available for Fall 2025.</h3>-->

<h3>Research Interests</h3>
<p>
I am broadly interested in machine learning algorithms that are well motivated and truly work in practice.
So far, my research topics include multi-modal/multi-view representation learning, speech and audio processing, optimization for machine learning, and applications.
</p>

<h3>Work Experience</h3>
<ul>
<li><p>Aug 2024--Current. Assistant Professor. University of Iowa.</p></li>
<li><p>Jan 2021--June 2024. Senior and Staff Research Scientist. Google.</p></li>
<li><p>Nov 2019--Dec 2020. Senior Research Scientist. Salesforce Research.</p></li>
<li><p>Oct 2017--Oct 2019. Senior Research Scientist. Amazon Alexa.</p></li>
<li><p>Jan 2014--Sep 2017. Postdoc Researcher. <a href="http://ttic.edu">Toyota Technological Institute at Chicago.</a>
	Advisors: <a href="http://ttic.uchicago.edu/~klivescu/">Karen Livescu</a> and <a href="http://ttic.uchicago.edu/~nati/">Nathan Srebro</a>.</p></li>
</ul>

<h3>Education</h3>
<ul>
<li><p>Aug 2008--Dec 2013. PhD, <a href="http://eecs.ucmerced.edu">EECS Department</a> at UC Merced. Advisor: <a href="http://faculty.ucmerced.edu/mcarreira-perpinan/index.html">Miguel A. Carreira-Perpinan</a>.</p></li>
<li><p>Sep 2005--June 2008. Master in Computer Science. Chengdu Institute of Computer Applications, Chinese Academy of Sciences. Chengdu, China.</p></li>
<li><p>Sep 2001--June 2005. Bachelor in Computer Science. Huazhong University of Science and Technology. Wuhan, China.</p></li>
</ul>

<h3>Teaching</h3>
<ul>
<li><p>Fall 2025. CS4420: Artificial Intelligence.</p></li>
<li><p>Spring 2025. CS4980: Deep Learning.</p></li>
<li><p>Fall 2024. CS4420: Artificial Intelligence.</p></li>
</ul>
	  
<h3>Selected Publications</h3>
Full publication list can be found in <a href=https://scholar.google.com/citations?user=O9djN1AAAAAJ&hl=en>google scholar</a>.
<ul>

<li><p>
Shanmuka Sadhu, Weiran Wang.
	Improving Audio Event Recognition with Consistency Regularization. <em>2025.</em> <br>
<font size="-1", color=red>We extend the use of consistency regularization to audio event recognition, and observe that stronger augmentations and multiple augmentations enhance the performance for small training set, and semi-supervised learning with consistency loss on unlabeled data leads to further gain.</font><br>
[<a href="https://arxiv.org/abs/2509.10391">arXiv version</a>] </p></li>
	
<li><p>
Yiyang Shen, Weiran Wang.
	A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels. <em>2025.</em> <br>
<font size="-1", color=red>We propose a semi-supervised generative model that utilizes both labeled and unlabeled samples in a unified framework, and maximize the likelihood of unlabeled samples to learn a latent space shared with the information bottleneck on labeled data.</font><br>
[<a href="https://arxiv.org/abs/2508.11180">arXiv version</a>] </p></li>
	
<li><p>
Wanting Huang, Weiran Wang.
	A Neural Model for Contextual Biasing Score Learning and Filtering. <em>ASRU, 2025.</em> <br>
<font size="-1", color=red>We use an attention-based biasing decoder to produce scores for candidate phrases based on acoustic information extracted by an ASR encoder, which can be used to filter out unlikely phrases and to calculate bonus for shallow-fusion biasing.</font><br>
[<a href="">arXiv version</a>] </p></li>

	
<li><p>
Weiran Wang, Zelin Wu, Diamantino Caseiro, Tsendsuren Munkhdalai, Khe Chai Sim, Pat Rondon, Golan Pundak, Gan Song, Rohit Prabhavalkar, Zhong Meng, Ding Zhao, Tara Sainath, Pedro Moreno Mengibar.
	Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm.  <em>Interspeech, 2024.</em> <br>
<font size="-1", color=red>We propose a GPU/TPU implementation of search-based ASR biasing, which was traditionally done with weighted finite state transducers (WFSTs), based on the equivalent KMP string matching algorithm.
	Our implementation reduces to a few carefully parallelized loops in deep learning frameworks. The gain from search-based biasing is additive to that of model-based biasing.</font><br>
[<a href="https://arxiv.org/abs/2310.00178">arXiv version</a>] </p></li>

<li><p>
Weiran Wang, Rohit Prabhavalkar, Haozhe Shan, Zhong Meng, Dongseong Hwang, Qiujia Li, Khe Chai Sim, Bo Li, James Qin, Xingyu Cai, Adam Stooke, Chengjian Zheng, Yanzhang He, Tara Sainath, Pedro Moreno Mengibar.
	Massive End-to-end Speech Recognition Models with Time Reduction. <em>NAACL, 2024.</em> <br>
<font size="-1", color=red>For massive ASR models (including CTC and RNN-T), it is possible to reduce the encoder output frame rate significantly with funnel pooling, without sacrificing recognition accuracy.</font><br>
[<a href="https://aclanthology.org/2024.naacl-long.344/">link</a>] </p></li>


<li><p>Weiran Wang, Ke Hu, and Tara Sainath. Deliberation of Streaming RNN-Transducer by Non-autoregressive Decoding.  <em> ICASSP, 2022.</em> <br>
<font size="-1", color=red>We can improve the ASR accuracy of a small streaming RNN-T model with non-AR decoding by CTC, taking into account both acoustic features and label dependency of initial hypothesis.</font><br>	
[<a href="https://arxiv.org/abs/2112.11442">arXiv version</a>] </p></li>
    
<li><p>Qi Lyu, Xiao Fu, Weiran Wang, and Songtao Lu. Latent Correlation-Based Multiview Learning and Self-Supervision: A Unifying Perspective. <em> ICLR, 2022.</em> <br>
<font size="-1", color=red>We give an example of provable separation of shared vs private components for multi-view self-supervised learning, based on CCA-type matching + reconstruction + group independence promoting regularization.</font><br>
[<a href="https://arxiv.org/abs/2106.07115">arXiv version</a>] </p></li> 
    
<li><p>Junwen Bai, Weiran Wang, and Carla Gomes. Contrastively Disentangled Sequential Variational Autoencoder. <em> NeurIPS, 2021.</em> <br>
<font size="-1", color=red>We use contrastive mutual information estimation for separating the static component and dynamic component of sequence data.</font><br>
[<a href="https://arxiv.org/abs/2110.12091">arXiv version</a>] [<a href="https://github.com/JunwenBai/C-DSVAE">implementation</a>] </p></li> 
    
<li><p>Junwen Bai, Weiran Wang, Yingbo Zhou, and Caiming Xiong. Representation Learning for Sequence Data with Deep Autoencoding Predictive Components. <em>International Conference on Learning Representations (ICLR), 2021.</em> <br>
<font size="-1", color=red>We use predictive information as regularization for representation learning with sequence encoders.</font><br>
[<a href="https://arxiv.org/abs/2010.03135">arXiv version</a>] [<a href="https://github.com/JunwenBai/DAPC">implementation</a>] </p></li> 

<li><p>Weiran Wang, Guangsen Wang, Aadyot Bhatnagar, Yingbo Zhou, Caiming Xiong, and Richard Socher. An investigation of phone-based subword units for end-to-end speech recognition. <em>Interspeech, 2020.</em> <br>
<font size="-1", color=red>We are the first to investigate phone-based subword units for end-to-end speech recognition.
	For Switchboard, our phone-based BPE system achieves 6.8%/14.4% word error rate (WER) on the Switchboard/CallHome
	portion of the test set while joint decoding achieves 6.3%/13.3% WER.
On Fisher + Switchboard, joint decoding leads to 4.9%/9.5% WER, setting new milestones for telephony speech recognition. </font><br>
[<a href="https://arxiv.org/abs/2004.04290">arXiv version</a>] [<a href="https://github.com/salesforce/transformerasr">implementation</a>] </p></li>
    
<li><p>Yang Chen, Weiran Wang, and Chao Wang. Semi-supervised ASR by End-to-end Self-training. <em>Interspeech, 2020.</em> <br>
<font size="-1", color=red>We are one of the first to perform on-the-fly self-training for end-to-end ASR, where we use pseudo-labels on clean data as targets for the same model on perturbed data. </font><br>
[<a href="https://arxiv.org/abs/2001.09128">arXiv version</a>] </p></li>
    
<li><p>Weiran Wang, Qingming Tang, and Karen Livescu. Unsupervised Pre-training of Bidirectional Speech Encoders via Masked Reconstruction. <em>ICASSP, 2020.</em><br>
<font size="-1", color=red>We are one of the first to perform (BERT-style) masked reconstruction for pre-training speech encoders.</font><br>
[<a href="https://arxiv.org/abs/2001.10603">arXiv version</a>] </p></li>
    
<li><p>Chao Gao, Dan Garber, Nathan Srebro, Jialei Wang, Weiran Wang (by &alpha;-&beta; order). Stochastic Canonical Correlation Analysis. Accepted by <em>Journal of Machine Learning Research.</em><br>
<font size="-1", color=red>We provide tight sample complexity analysis of streaming CCA that matches the statistical limit for Gaussian inputs.</font><br>
[<a href="https://arxiv.org/abs/1702.06533">arXiv version</a>] </p></li>

<li><p>Weiran Wang and Nathan Srebro. Stochastic Nonconvex Optimization with Large Minibatches. <em>Algorithmic Learning Theory (ALT), 2019.</em><br>
<font size="-1", color=red>We apply proximal point algorithm to non-convex population objective, which boils down to solving a sequence of convex problems,
	and show benefits of larger minibatch size when the loss is not too nonconvex.</font><br>
[<a href="https://arxiv.org/abs/1709.08728">arXiv version</a>] </p></li>

<li><p>Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep Variational Canonical Correlation Analysis. <br>
<font size="-1", color=red>We extend the probabilistic interpretation of CCA to use deep generative models, and demonstrate the separation of shared components 
	from private components for multi-view data including audio + articulation, image + text.</font><br>
[<a href="https://arxiv.org/abs/1610.03454">arXiv version</a>] </p></li>
	
<li><p>Jialei Wang<sup>*</sup>, Weiran Wang<sup>*</sup>, and Nathan Srebro. Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch Prox.
	<em>Conference On Learning Theory (COLT), 2017.</em> <br>
<font size="-1", color=red>We apply the proximal point algorithm to stochastic optimization, which leads to a sequence of regularized ERM problems solved in a distributed setup;
		this approach offers trade offs between batch size and communication rounds.</font><br>
[<a href="https://arxiv.org/abs/1702.06269">arXiv version</a>] </p></li>

<li><p>Weiran Wang<sup>*</sup>, Jialei Wang<sup>*</sup>, Dan Garber, and Nathan Srebro. Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis. <em>Advances in Neural Information Processing Systems (NIPS), 2016.</em><br>
<font size="-1", color=red>We provide one of the first stochastic optimization algorithms for CCA with global convergence guarantee, based on the eigen-value structure of CCA, where each update makes use of a single data point.</font><br>
[<a href="https://arxiv.org/abs/1604.01870">arXiv version</a>] </p></li>

<li><p>Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On Deep Multi-View Representation Learning. <em>International Conference on Machine Learning (ICML), 2015. </em><br>
<font size="-1", color=red>While the deep CCA objective couples all training sample together, we demonstrate that stochastic optimization works well with large minibatch size.
Deep CCA is a powerful model for extracting the shared component from multi-view data and we propose auto-encoding regularization for it.</font><br>
[<a href="http://ttic.edu/livescu/XRMB_data/full/README">XRMB dataset</a>] [<a href="https://bitbucket.org/qingming_tang/deep-canonical-correlation-analysis/src/master/">Tensorflow implementation!</a>]</p></li>
	
<!--<li><p>Weiran Wang and Miguel A. Carreira-Perpinan. Manifold Blurring Mean Shift Algorithms for Manifold Denoising. <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.</em> </p></li>-->
</ul>


<h3>Students and Interns</h3>
<ul>
<li><p><a href="">Yiyang Shen</a>. PhD student, 2025-.</p></li>
<li><p><a href="">Wanting Huang</a>. PhD student, 2025-.</p></li>
<li><p><a href="">Shanmuka Sadhu</a>. Undergraduate at Rutgers University, <a href="https://homepage.divms.uiowa.edu/~kleimn/reu/">NSF REU Computing for Health and Well-being</a>, 2025.</p></li>
<li><p><a href="https://hzshan.github.io">Haozhe Shan</a>. Intern at Google, 2023.</p></li>
<li><p><a href="https://junwenbai.github.io">Junwen Bai</a>. Intern at Salesforce Research, 2020.</p></li>
<li><p><a href="https://edchengg.github.io">Yang Chen</a>. Intern at Amazon Alexa, 2019.</p></li>
</ul>


<!--<h3>Teaching Assistantship</h3>
<ul>
<li><strike>CSE20: Introduction to Computing I, Fall 2012</strike></li>
<li><strike>CSE30: Introduction to Computer Science and Engineering I, Fall 2011</strike></li>
<li><strike>CSE176/EECS276: Machine Learning, Fall 2010</strike></li>
<ul><li><a href="TA/lecture.pdf">(August 27)Classification</a></li><li><a href="TA/P1.pdf">(September 9)Lab Assignment: Principal Component Analysis</a></li></ul>
<li><strike>CSE31: Introduction to Computer Science and Engineering II, Spring 2010</strike></li>
<li><strike><a href="http://johnewart.net/teaching/cse31/">CSE31: Introduction to Computing II, Spring 2010</a></strike></li>
	<ul>
	<li><em>Office hour: Wednesday 3:00-4:00pm, AOB 142</em></li>
	<li><a href="TA/gdb5-refcard.pdf">GDB Reference Card</a></li>
	<li><a href="http://www.scss.tcd.ie/John.Waldron/itral/spim_ref.html">SPIM Quick Reference</a></li>
	<li><a href="TA/minishell.html">Hints for Minishell</a></li>
	<li><a href="http://www.cs.cf.ac.uk/Dave/C/">A good referece for Interprocess Comunication (IPC)</a></li>
	<li><a href="http://beej.us/guide/bgnet/output/html/multipage/index.html">A good reference for Network Programming using Sockets</a></li>
	<li><a href="http://www.tenouk.com/Bufferoverflowc/Bufferoverflow3.html">What's going on in a stack frame</a></li>
	<li><a href="http://tldp.org/HOWTO/NCURSES-Programming-HOWTO/">NCURSES Programming HOWTO</a></li>
	</ul>
<li><strike>CSE31: Introduction to Computer Science and Engineering II, Spring 2009</strike></li>
<li><strike>CSE30: Introduction to Computer Science and Engineering I, Spring 2009</strike></li>
	<ul>
	<li><a href="TA/minesweeper.html">How to use stack for Minesweepers</a></li>
	</ul>
</ul>
-->

<h3>Some Links</h3>
<ul>
<!--<li><a href="manuals.html">Frequently used manuals</a></li>-->
<li><a href="tips.html">Tips for academic life from others</a></li>
</ul>
	  
<hr>

<address> Send email to
<a href="mailto: weiran-wang@uiowa.edu">Weiran Wang</a>.
</address>

<script 
  type="text/JavaScript" 
  language="JavaScript">
//
// format date as dd-mmm-yy
// example: 12-Jan-99
//
function date_ddmmmyy(date)
{
  var d = date.getDate();
  var m = date.getMonth() + 1;
  var y = date.getYear();

  // handle different year values 
  // returned by IE and NS in 
  // the year 2000.
  if(y >= 2000)
  {
    y -= 2000;
  }
  if(y >= 100)
  {
    y -= 100;
  }

  // could use splitString() here 
  // but the following method is 
  // more compatible
  var mmm = 
    ( 1==m)?'Jan':( 2==m)?'Feb':(3==m)?'Mar':
    ( 4==m)?'Apr':( 5==m)?'May':(6==m)?'Jun':
    ( 7==m)?'Jul':( 8==m)?'Aug':(9==m)?'Sep':
    (10==m)?'Oct':(11==m)?'Nov':'Dec';

  return "" +
    (d<10?"0"+d:d) + "-" +
    mmm + "-" +
    (y<10?"0"+y:y);
}


//
// get last modified date of the 
// current document.
//
function date_lastmodified()
{
  var lmd = document.lastModified;
  var s   = "Unknown";
  var d1;

  // check if we have a valid date
  // before proceeding
  if(0 != (d1=Date.parse(lmd)))
  {
    s = "" + date_ddmmmyy(new Date(d1));
  }

  return s;
}

//
// finally display the last modified date
// as DD-MMM-YY
//
document.write("This page was updated on " + date_lastmodified() + ".");

//
</script>
    
  </body>
</html>
